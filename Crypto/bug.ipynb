{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 100\n",
    "future_pre = 1\n",
    "ratio_to_pre = \"BCH-USD\"\n",
    "\n",
    "epochs = 20\n",
    "b_size = 100\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{future_pre}-pre-{int(time.time())}\"\n",
    "\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "ratios = [\"BTD-USD\", \"LTC-USD\", \"ETH-USD\", \"BCH-USD\"]\n",
    "for ratio in ratios:\n",
    "    dataset = f\"crypto_data/{ratio}.csv\"\n",
    "    \n",
    "    df = pd.read_csv(\"crypto_data/LTC-USD.csv\", names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\n",
    "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_vloume\"}, inplace=True)\n",
    "    \n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_vloume\"]]\n",
    "    \n",
    "    if len(main_df) == 0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)\n",
    "        \n",
    "        \n",
    "main_df['future'] = main_df[f\"{ratio_to_pre}_close\"].shift(-future_pre)\n",
    "main_df['target'] = list(map(classify, main_df[f\"{ratio_to_pre}_close\"], main_df[\"future\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess\n",
    "def preprocess_df(df):\n",
    "    df = df.drop('future', 1)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col != \"target\":\n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace=True)\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "            \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])\n",
    "\n",
    "    #balance\n",
    "    buys = []\n",
    "    sells = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "            \n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "\n",
    "    lower = min(len(buys), len(sells))\n",
    "    \n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    sequential_data = buys+sells\n",
    "    \n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        x.append(seq)\n",
    "        y.append(target)\n",
    "        \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create normalized and ready-to-train/test data\n",
    "times = sorted(main_df.index.values)\n",
    "last_5pct = times[-int(0.05*len(times))]\n",
    "\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "main_df = main_df[(main_df.index < last_5pct)]\n",
    "\n",
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70870 samples, validate on 3318 samples\n",
      "Epoch 1/20\n",
      "70870/70870 [==============================] - 34s 478us/sample - loss: 0.7182 - accuracy: 0.5131 - val_loss: 0.6899 - val_accuracy: 0.5356\n",
      "Epoch 2/20\n",
      "70870/70870 [==============================] - 30s 427us/sample - loss: 0.6902 - accuracy: 0.5347 - val_loss: 0.6857 - val_accuracy: 0.5561\n",
      "Epoch 3/20\n",
      "70870/70870 [==============================] - 30s 429us/sample - loss: 0.6822 - accuracy: 0.5562 - val_loss: 0.6637 - val_accuracy: 0.6052\n",
      "Epoch 4/20\n",
      "70870/70870 [==============================] - 30s 429us/sample - loss: 0.6703 - accuracy: 0.5826 - val_loss: 0.6464 - val_accuracy: 0.6239\n",
      "Epoch 5/20\n",
      "70870/70870 [==============================] - 30s 422us/sample - loss: 0.6642 - accuracy: 0.5950 - val_loss: 0.6461 - val_accuracy: 0.6290\n",
      "Epoch 6/20\n",
      "70870/70870 [==============================] - 30s 420us/sample - loss: 0.6611 - accuracy: 0.5993 - val_loss: 0.6555 - val_accuracy: 0.6242\n",
      "Epoch 7/20\n",
      "70870/70870 [==============================] - 30s 428us/sample - loss: 0.6605 - accuracy: 0.6001 - val_loss: 0.6448 - val_accuracy: 0.6347\n",
      "Epoch 8/20\n",
      "70870/70870 [==============================] - 30s 418us/sample - loss: 0.6594 - accuracy: 0.6021 - val_loss: 0.6462 - val_accuracy: 0.6474\n",
      "Epoch 9/20\n",
      "70870/70870 [==============================] - 31s 444us/sample - loss: 0.6584 - accuracy: 0.6038 - val_loss: 0.6538 - val_accuracy: 0.6190\n",
      "Epoch 10/20\n",
      "70870/70870 [==============================] - 33s 467us/sample - loss: 0.6575 - accuracy: 0.6051 - val_loss: 0.6409 - val_accuracy: 0.6383\n",
      "Epoch 11/20\n",
      "70870/70870 [==============================] - 31s 444us/sample - loss: 0.6567 - accuracy: 0.6042 - val_loss: 0.6518 - val_accuracy: 0.6383\n",
      "Epoch 12/20\n",
      "70870/70870 [==============================] - 33s 459us/sample - loss: 0.6568 - accuracy: 0.6064 - val_loss: 0.6482 - val_accuracy: 0.6251\n",
      "Epoch 13/20\n",
      "70870/70870 [==============================] - 34s 484us/sample - loss: 0.6552 - accuracy: 0.6066 - val_loss: 0.6562 - val_accuracy: 0.6245\n",
      "Epoch 14/20\n",
      "70870/70870 [==============================] - 33s 464us/sample - loss: 0.6539 - accuracy: 0.6091 - val_loss: 0.6411 - val_accuracy: 0.6453\n",
      "Epoch 15/20\n",
      "70870/70870 [==============================] - 33s 464us/sample - loss: 0.6540 - accuracy: 0.6095 - val_loss: 0.6402 - val_accuracy: 0.6404\n",
      "Epoch 16/20\n",
      "70870/70870 [==============================] - 33s 471us/sample - loss: 0.6536 - accuracy: 0.6099 - val_loss: 0.6636 - val_accuracy: 0.6184\n",
      "Epoch 17/20\n",
      "70870/70870 [==============================] - 33s 465us/sample - loss: 0.6524 - accuracy: 0.6120 - val_loss: 0.6474 - val_accuracy: 0.6531\n",
      "Epoch 18/20\n",
      "70870/70870 [==============================] - 32s 445us/sample - loss: 0.6525 - accuracy: 0.6109 - val_loss: 0.6541 - val_accuracy: 0.5989\n",
      "Epoch 19/20\n",
      "70870/70870 [==============================] - 34s 478us/sample - loss: 0.6512 - accuracy: 0.6131 - val_loss: 0.6552 - val_accuracy: 0.6293\n",
      "Epoch 20/20\n",
      "70870/70870 [==============================] - 34s 474us/sample - loss: 0.6508 - accuracy: 0.6138 - val_loss: 0.6474 - val_accuracy: 0.6371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cd11491bc8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2)) #0.1\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "checkpoint_filepath = './models/BCH-USD/cp-{epoch:02d}.ckpt'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                               save_weights_only=True,\n",
    "                                                               monitor='val_accuracy',\n",
    "                                                               mode='max',\n",
    "                                                               save_best_only=True)\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=b_size, epochs=epochs, validation_data=(validation_x, validation_y), callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3318/3318 - 1s - loss: 0.6474 - accuracy: 0.6531\n",
      "Restored model, accuracy: 65.31%\n"
     ]
    }
   ],
   "source": [
    "# Loads the weights\n",
    "checkpoint_path = './models/BCH-USD/cp-17.ckpt'\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss,acc = model.evaluate(validation_x, validation_y, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py 3.7 tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
